<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>网络结构 on 巴巴变的博客</title>
        <link>https://blog.bugxch.top/tags/%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84/</link>
        <description>Recent content in 网络结构 on 巴巴变的博客</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>bugxch</copyright><atom:link href="https://blog.bugxch.top/tags/%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84/index.xml" rel="self" type="application/rss+xml" /><item>
        <title></title>
        <link>https://blog.bugxch.top/wucai/2024/04/wucai-2024-04-10-h8f59d8/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://blog.bugxch.top/wucai/2024/04/wucai-2024-04-10-h8f59d8/</guid>
        <description>&lt;h2 id=&#34;swin-transformer&#34;&gt;Swin Transformer&lt;/h2&gt;
&lt;p&gt;#网络结构&lt;/p&gt;
&lt;h2 id=&#34;页面笔记&#34;&gt;页面笔记&lt;/h2&gt;
&lt;p&gt;#网络结构 非常不错的介绍swin transformer的文章&lt;/p&gt;
&lt;h2 id=&#34;划线列表&#34;&gt;划线列表&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;img src=&#34;https://amaarora.github.io/images/swin-transformer-block.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt; &lt;a class=&#34;link&#34; href=&#34;https://amaarora.github.io/posts/2022-07-04-swintransformerv1.html#WCREFX-3340100&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Link&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;#网络结构 swin transformer结构&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;img src=&#34;https://amaarora.github.io/images/window-partition.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt; &lt;a class=&#34;link&#34; href=&#34;https://amaarora.github.io/posts/2022-07-04-swintransformerv1.html#WCREFX-3444486&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Link&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;#网络结构 swin transformer的结构&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;While the Transformer architecture before this paper had proved to be performing better than CNNs on the ImageNet dataset, it was yet to be utilised as a general purpose backbone for other tasks such as object detection &amp;amp; semantic segmentation. This paper solves that problem and Swin Transformers can capably serve as general purpose backbones for computer vision. &lt;a class=&#34;link&#34; href=&#34;https://amaarora.github.io/posts/2022-07-04-swintransformerv1.html#WCREFX-3476556&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Link&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;transformer证明了在更通用的物体检测以及分割上也很不错&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;img src=&#34;https://amaarora.github.io/images/swin-transformer.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt; &lt;a class=&#34;link&#34; href=&#34;https://amaarora.github.io/posts/2022-07-04-swintransformerv1.html#WCREFX-3476579&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;The global computation leads to quadratic complexity with respect to the number of tokens, making it unsuitable for many vision problems requiring an immense set of tokens for dense prediction or to represent a high-resolution image &lt;a class=&#34;link&#34; href=&#34;https://amaarora.github.io/posts/2022-07-04-swintransformerv1.html#WCREFX-3477153&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Link&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;global attention计算复杂度是token的平方，复杂度很高&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;全文剪藏&#34;&gt;全文剪藏&lt;/h2&gt;
</description>
        </item>
        <item>
        <title></title>
        <link>https://blog.bugxch.top/wucai/2024/04/wucai-2024-04-19-h8f2768/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://blog.bugxch.top/wucai/2024/04/wucai-2024-04-19-h8f2768/</guid>
        <description>&lt;h2 id=&#34;the-annotated-transformer&#34;&gt;The Annotated Transformer&lt;/h2&gt;
&lt;p&gt;#星标 #网络结构&lt;/p&gt;
&lt;h2 id=&#34;页面笔记&#34;&gt;页面笔记&lt;/h2&gt;
&lt;p&gt;#网络结构 有注解的transformer的论文解读&lt;/p&gt;
&lt;h2 id=&#34;划线列表&#34;&gt;划线列表&lt;/h2&gt;
&lt;h2 id=&#34;全文剪藏&#34;&gt;全文剪藏&lt;/h2&gt;
</description>
        </item>
        <item>
        <title></title>
        <link>https://blog.bugxch.top/wucai/2024/04/wucai-2024-04-19-h8f276b/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://blog.bugxch.top/wucai/2024/04/wucai-2024-04-19-h8f276b/</guid>
        <description>&lt;h2 id=&#34;the-annotated-transformer&#34;&gt;The Annotated Transformer&lt;/h2&gt;
&lt;p&gt;#星标 #网络结构&lt;/p&gt;
&lt;h2 id=&#34;页面笔记&#34;&gt;页面笔记&lt;/h2&gt;
&lt;p&gt;#网络结构 transformer的比较注解&lt;/p&gt;
&lt;h2 id=&#34;划线列表&#34;&gt;划线列表&lt;/h2&gt;
&lt;h2 id=&#34;全文剪藏&#34;&gt;全文剪藏&lt;/h2&gt;
</description>
        </item>
        <item>
        <title></title>
        <link>https://blog.bugxch.top/wucai/2024/04/wucai-2024-04-19-h8f27e9/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://blog.bugxch.top/wucai/2024/04/wucai-2024-04-19-h8f27e9/</guid>
        <description>&lt;h2 id=&#34;u-net-a-pytorch-implementation-in-60-lines-of-code&#34;&gt;U-Net A PyTorch Implementation in 60 lines of Code&lt;/h2&gt;
&lt;p&gt;#星标 #网络结构&lt;/p&gt;
&lt;h2 id=&#34;页面笔记&#34;&gt;页面笔记&lt;/h2&gt;
&lt;p&gt;#网络结构 使用pytorch的60行完成一个Unet&lt;/p&gt;
&lt;h2 id=&#34;划线列表&#34;&gt;划线列表&lt;/h2&gt;
&lt;h2 id=&#34;全文剪藏&#34;&gt;全文剪藏&lt;/h2&gt;
</description>
        </item>
        
    </channel>
</rss>
